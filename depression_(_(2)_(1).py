# -*- coding: utf-8 -*-
"""Depression_(_(2)_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1giodpix2mJfcrlT7V1Yva42L73Mua0gz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

"""## Data Cleaning"""

import pandas as pd            # pandas is used to load, manipulate, and save tabular data (CSV files)
import re                      # re is used for pattern matching and text cleaning using regular expressions


# This function removes Bengali characters from a given text string
# WHY: Some column names contain Bengali text which can cause issues in analysis and ML models
# HOW: Uses Unicode range of Bengali characters and replaces them with an empty string
def remove_bengali(text):
    return re.sub(r'[\u0980-\u09FF]+', '', text).strip()   # Remove Bengali characters and extra spaces


# Load the original dataset from CSV file into a pandas DataFrame
# WHY: This is the raw dataset that needs to be cleaned before analysis
df = pd.read_csv('data.csv')


# Clean column names by removing Bengali text
# WHAT: Applies remove_bengali() function to every column name
# HOW: List comprehension loops through all columns and cleans them one by one
df.columns = [remove_bengali(col) for col in df.columns]


# Save the cleaned dataset into a new CSV file
# WHY: Keeps the original dataset unchanged and stores a reusable cleaned version
df.to_csv('data_cleaned.csv', index=False)


# Reload the cleaned dataset for further processing and exploration
# WHY: Ensures that all further operations use the cleaned version only
df_cleaned = pd.read_csv('data_cleaned.csv')


# Rename long and complex column names into short, meaningful names
# WHY: Short names are easier to use during feature selection and model building
df = df.rename(columns={
    "Depression Level (BDI-II) (20 items)": "Depression_BDI",
    "Depression Level (PHQ-9 items)": "Depression_PHQ9",
    "Depression Level (CES-D) (20 items)": "Depression_CESD"
})


# Remove all types of brackets and the text inside them from column names
# HOW: Regex pattern removes anything inside parentheses
df.columns = df.columns.str.replace(r"\(.*?\)", "", regex=True)


# Remove leading and trailing spaces from column names
# WHY: Extra spaces can cause mismatches while accessing columns
df.columns = df.columns.str.strip()


# Replace multiple spaces with a single space in column names
# WHY: Ensures consistent formatting and improves readability
df.columns = df.columns.str.replace(r"\s+", " ", regex=True)


# Print final cleaned column names to verify all cleaning steps worked correctly
print(df.columns.tolist())

"""Imputation"""

import pandas as pd
import numpy as np
import missingno as msno
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer


df = pd.read_csv("data_cleaned.csv")  # üëà replace with your actual file name

print("Initial dataset shape:", df.shape)
print("\nMissing values per column:\n", df.isnull().sum())

threshold = 0.75
cols_to_keep = df.columns[df.isnull().mean() < threshold]
df = df[cols_to_keep]

print("\n‚úÖ Columns retained (less than 75% missing):")
print(df.columns.tolist())

def auto_impute(df):
    df_imputed = df.copy()

    # Separate numeric and categorical columns
    num_cols = df_imputed.select_dtypes(include=[np.number]).columns
    cat_cols = df_imputed.select_dtypes(exclude=[np.number]).columns

    # --- CATEGORICAL COLUMNS ---
    for col in cat_cols:
        missing_pct = df_imputed[col].isnull().mean()
        if missing_pct > 0.3:
            print(f"‚ö†Ô∏è Dropping categorical column '{col}' ({missing_pct*100:.1f}% missing)")
            df_imputed.drop(col, axis=1, inplace=True)
        else:
            # Mode imputation
            imputer = SimpleImputer(strategy="most_frequent")
            df_imputed[[col]] = imputer.fit_transform(df_imputed[[col]])

    # --- NUMERIC COLUMNS ---
    low_missing = [col for col in num_cols if df_imputed[col].isnull().mean() <= 0.3]
    high_missing = [col for col in num_cols if df_imputed[col].isnull().mean() > 0.3]

    # Single (Median) imputation for low-missing columns
    if low_missing:
        median_imputer = SimpleImputer(strategy="median")
        df_imputed[low_missing] = median_imputer.fit_transform(df_imputed[low_missing])

    # Multiple (MICE) imputation for high-missing numeric columns
    if high_missing:
        print(f"üß† Applying multiple imputation (MICE) for: {high_missing}")
        mice_imputer = IterativeImputer(random_state=42)
        df_imputed[high_missing] = mice_imputer.fit_transform(df_imputed[high_missing])

    return df_imputed


df_cleaned = auto_impute(df)


plt.subplot(1,2,1)
msno.matrix(df)
plt.title("Before Imputation")
plt.subplot(1,2,2)
msno.matrix(df_cleaned)
plt.title("After Imputation")
plt.subplots_adjust(wspace=0.4)
plt.show()

print("\nMissing values after imputation:\n", df_cleaned.isnull().sum())
print("\nFinal dataset shape:", df_cleaned.shape)


df_cleaned.to_csv("data_imputed.csv", index=False)
print("\n‚úÖ Cleaned dataset saved as 'data_imputed.csv'")

"""correlation"""

import pandas as pd
import numpy as np
df = pd.read_csv("data_imputed.csv")  # üëà use your dataset

numeric_df = df.select_dtypes(include=[np.number])
corr_matrix = numeric_df.corr().abs()

# Get upper triangle (to avoid duplicates)
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Collect all correlation pairs
corr_pairs = []
for col1 in upper_tri.columns:
    for col2 in upper_tri.index:
        corr_val = upper_tri.loc[col2, col1]
        if not pd.isna(corr_val):
            corr_pairs.append((col1, col2, corr_val))

# Sort by correlation strength (highest first)
corr_pairs_sorted = sorted(corr_pairs, key=lambda x: -x[2])

# Display results
print("\nüîπ All numeric column correlation pairs:\n")
for col1, col2, corr in corr_pairs_sorted:
    print(f"{col1:<25} ‚Üî {col2:<25} : {corr:.3f}")

print(numeric_df.shape)  # How many numeric columns do you actually have?
print(numeric_df.describe())  # See if the numeric columns have variation

"""# Label Encoding"""

le = LabelEncoder()

categorical_cols = [
    'Gender', 'Relationship Status', 'Academic Status',
    'Do you work as well as Study?(   ?)',
    'Residential Area ( )',
    'Social Economic Status (- )',
    'Do you feel any financial pressure?(      ?)',
    'Does the participant have any debts?(    ?)',
    'Are you satisfied with your current living environment? (      ?)'
]

for col in categorical_cols:
    df[col] = le.fit_transform(df[col].astype(str))

df["How often do you feel isolated from others?(      ?)"].unique()

# Mapping
ordinal_map = {
    "Never": 0,
    "Rarely": 1,
    "Sometimes": 2,
    "Often": 3
}


# All 8 columns including the one with typo
ordinal_columns = [
    "How often do you feel isolated from others?(      ?)",
    "How frequently do you feel there‚Äôs no one you can rely on for support?(               ?)",
    "How frequently do you experience feelings of loneliness? (       ?)",
    "How frequently do you feel aligned with the emotions or thoughts of the people around you? (             ?)",
    "How frequently do you perceive that people are around you but not genuinely present with you? (                ?)",
    "To what extent do you experience your relationships with others as being unimportant?(         ?)",
    "How often do you feel like you don't have anyone to share your feelings with? (             ?)",
    "How much do you feel left out in social situations? (      ?)"]



# Fix common typos
typo_corrections = {
    "Sometimess": "Sometimes"
}

# Apply standardization + typo correction + mapping
for col in ordinal_columns:
    df[col] = df[col].astype(str).str.strip().str.title()
    df[col] = df[col].replace(typo_corrections)
    df[col] = df[col].map(ordinal_map)

for col in ordinal_columns:
    print(f"\nUnique values in '{col}':")
    print(df[col].unique())

df = df.rename(columns={
    "Depression Level (BDI-II) (20 items)": "Depression_BDI",
    "Depression Level (PHQ-9 items)": "Depression_PHQ9",
    "Depression Level (CES-D) (20 items)": "Depression_CESD",
    "Loneliness Level (UCLA-8 items)": "Loneliness_Level"
})

depression_map = {
    "no Depressive" : 0,
    "Minimal Depression": 1,
    "Mild Depression": 2,
    "Moderate Depression": 3,
    "Severe Depression": 4
}

loneliness_map = {
    "Low degree of loneliness": 0,
    "Moderate degree of loneliness": 1,
    "Moderately high degree of loneliness": 2,
    "High degree of loneliness": 3
}

df["Depression_PHQ9"] = df["Depression_PHQ9"].astype(str).map(depression_map)
df["Depression_BDI"] = df["Depression_BDI"].astype(str).map(depression_map)
df["Depression_CESD"] = df["Depression_CESD"].astype(str).map(depression_map)
df["Loneliness_Level"] = df["Loneliness_Level"].astype(str).map(loneliness_map)

# Convert 'Yes'/'No' type columns to binary
yes_no_map = {'Yes': 1, 'No': 0}
binary_cols = [
    'Have you recently lost someone close to you? (        )',
    'You are actively engaging as a participant in physical exertion.(      ‡•§)',
    'Are you afflicted by any significant ailments?(     ?")',
    'Are you currently on any prescribed medication?(      ?)',
    'Are you accustomed to smoking?(   ?)',
    'Do you consume alcoholic beverages?(     ?)',
    'Do you have current workload or academic demands?(        ?)',
    'Have you recently entertained any suicidal or self-harming thoughts?(     -      ?)',
    'Whether the participant has recently felt abused (physically, emotionally, Sexual Harassment) or Not (   (, ,  )    )',



]

for col in categorical_cols:
    if col in df.columns:
        if df[col].notna().sum() > 0:
            df[col] = le.fit_transform(df[col].astype(str))
            print(f"Encoded column: {col}")
        else:
            print(f"Skipped {col} (all values are NaN)")
    else:
        print(f"Column not found: {col}")

df["What is your average nightly sleep duration in hours?(       ?)"] = (
    df["What is your average nightly sleep duration in hours?(       ?)"]
    .astype(str)                        # ‚úÖ Convert all values to string first
    .str.extract(r'(\d+\.?\d*)')        # ‚úÖ Extract numeric part
    .astype(float)                      # ‚úÖ Convert extracted text to number
)

# Encode all remaining object columns
label_enc = LabelEncoder()
for col in df.select_dtypes(include='object').columns:
    df[col] = label_enc.fit_transform(df[col])

"""# Exploratory Data Analysis"""

df[binary_cols].info()

df.info()

df.head()

"""**unique** answers given for depression_PHQ9"""

# Check unique values in the column
unique_values = df['Depression_PHQ9'].unique()
print("Unique values:", unique_values)

# Count how many unique categories there are
num_categories = df['Depression_PHQ9'].nunique()
print("Number of categories:", num_categories)

print(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")

df.isnull().values.any()#null values

df.isnull().sum()#null columns

# Check total number of duplicate rows
duplicate_rows = df.duplicated().sum()
print(f"Number of duplicate (redundant) rows: {duplicate_rows}")

# Display the actual duplicate rows (if any)
if duplicate_rows > 0:
    print("\nDuplicate rows:")
    display(df[df.duplicated()])
else:
    print("\nNo duplicate rows found.")

# Check for duplicate values in each column
for col in df.columns:
    duplicate_values = df[col][df[col].duplicated()]
    if not duplicate_values.empty:
        print(f"\nDuplicate values found in column '{col}':")
        print(duplicate_values.unique())  # shows which values are duplicated
        print(f"Total duplicates in '{col}': {duplicate_values.shape[0]}")
    else:
        print(f"\nNo duplicate values found in column '{col}'.")

duplicate_counts = df.apply(lambda x: x.duplicated().sum())
print("\nNumber of duplicate values per column:")
print(duplicate_counts)

"""heatmap

"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
# Compute correlation matrix (only numeric columns)
corr_matrix = df.corr(numeric_only=True)

# ===============================
# 1Ô∏è‚É£ Set larger figure size
# ===============================
plt.figure(figsize=(40, 30))  # bigger figure for 63 columns

# ===============================
# 2Ô∏è‚É£ Plot heatmap with adjusted font size
# ===============================
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm',
            linewidths=0.5, cbar=True, square=True, annot_kws={"size": 8})

plt.title("Correlation Matrix Heatmap", fontsize=18)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.show()

df = df.drop(columns=[
    "I feel that people show me compassion. (      ‡•§)",
    "To what extent do you experience your relationships with others as being unimportant?(         ?)",
    "How frequently do you perceive that people are around you but not genuinely present with you? (                ?)"
])

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
# Compute correlation matrix (only numeric columns)
corr_matrix = df.corr(numeric_only=True)

# ===============================
# 1Ô∏è‚É£ Set larger figure size
# ===============================
plt.figure(figsize=(40, 30))  # bigger figure for 63 columns

# ===============================
# 2Ô∏è‚É£ Plot heatmap with adjusted font size
# ===============================
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm',
            linewidths=0.5, cbar=True, square=True, annot_kws={"size": 8})

plt.title("Correlation Matrix Heatmap", fontsize=18)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.show()

#Distribution of Depression and Loneliness levels

plt.figure(figsize=(14, 5))

# Depression
plt.subplot(1, 2, 1)
sns.countplot(x="Depression_PHQ9", data=df)
plt.title("Distribution of Depression (PHQ-9 Levels)")
plt.xlabel("Depression Level")
plt.ylabel("Count")

# Loneliness
plt.subplot(1, 2, 2)
sns.countplot(x="Loneliness_Level", data=df)
plt.title("Distribution of Loneliness Levels")
plt.xlabel("Loneliness Level")
plt.ylabel("Count")

plt.tight_layout()
plt.show()

"""##  Crosstab: Gender vs Depression"""

pd.crosstab(df['Gender'], df['Depression_PHQ9'], normalize='index') * 100

"""## Depression vs Gender & Relationship Status"""

plt.figure(figsize=(12, 5))

# Gender vs Depression
plt.subplot(1, 2, 1)
sns.boxplot(x="Gender", y="Depression_PHQ9", data=df)
plt.title("Depression Level by Gender")

# Relationship Status vs Depression
plt.subplot(1, 2, 2)
sns.boxplot(x="Relationship Status", y="Depression_PHQ9", data=df)
plt.title("Depression Level by Relationship Status")

plt.tight_layout()
plt.show()

"""## Pairplot Between Key Features"""

subset = df[[
    "Depression_PHQ9",
    "Loneliness_Level",
    "Age",
    "Academic Status",
    "Gender"
]]

sns.pairplot(subset, hue="Depression_PHQ9", palette="coolwarm")
plt.suptitle("Pairwise Relationships", y=1.02)
plt.show()

"""## Distribution Plot of Depression and Loneliness


"""

plt.figure(figsize=(10, 4))
sns.histplot(df["Depression_PHQ9"], kde=True, bins=4)
plt.title("Distribution of Depression (PHQ-9)")
plt.xlabel("Depression Level")
plt.show()

plt.figure(figsize=(10, 4))
sns.histplot(df["Loneliness_Level"], kde=True, bins=4, color="orange")
plt.title("Distribution of Loneliness")
plt.xlabel("Loneliness Level")
plt.show()

"""Feature importance tenchniques (REF)"""

from sklearn.feature_selection import SelectKBest, f_classif

# 1Ô∏è‚É£ Define X and Y
Y = df["Depression_PHQ9"]                             # target
X = df.drop(columns=["Depression_PHQ9"])              # features

# 2Ô∏è‚É£ Drop rows with null target
X_cleaned = X[Y.notna()]
Y_cleaned = Y.dropna()

# 3Ô∏è‚É£ Impute Nulls in X (required for SelectKBest)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="mean")
X_imputed = imputer.fit_transform(X_cleaned)

# 4Ô∏è‚É£ Scale features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# 5Ô∏è‚É£ Select top 10 features
selector = SelectKBest(score_func=f_classif, k=10)
X_selected = selector.fit_transform(X_scaled, Y_cleaned)

# 6Ô∏è‚É£ Get selected feature names
X_imputed_df = pd.DataFrame(X_imputed, columns=X_cleaned.columns)
selected_feature_names = X_imputed_df.columns[selector.get_support()]

print("Top 10 Selected Features:\n", selected_feature_names.tolist())

import pandas as pd
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import warnings
from sklearn.exceptions import ConvergenceWarning

# Suppress convergence warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Scale your feature data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed_df)

# Initialize Logistic Regression model
model_rfe = LogisticRegression(max_iter=5000, solver='saga', random_state=42)

# Initialize RFE
num_features_to_select = 10
rfe_selector = RFE(estimator=model_rfe, n_features_to_select=num_features_to_select, step=1)

# Fit RFE
rfe_selector.fit(X_scaled, Y_cleaned)

# Create a DataFrame with feature rankings
feature_ranking = pd.DataFrame({
    'Feature': X_imputed_df.columns,
    'Ranking (RFE)': rfe_selector.ranking_,
    'Selected (RFE)': rfe_selector.support_
})

# Filter features with Ranking = 1 (top features)
top_features = feature_ranking[feature_ranking['Ranking (RFE)'] == 1]

# Display only the feature names and ranking
print(f"Top {num_features_to_select} Features selected by RFE:")
print(top_features[['Feature', 'Ranking (RFE)']])

"""PC"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd

# ===============================
# 1Ô∏è‚É£ Standardize features
# ===============================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed_df)

# ===============================
# 2Ô∏è‚É£ Apply PCA
# ===============================
n_components = 2
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# ===============================
# 3Ô∏è‚É£ Create PCA DataFrame for visualization
# ===============================
pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(n_components)])
pca_df['Depression_PHQ9'] = Y_cleaned.reset_index(drop=True)

# ===============================
# 4Ô∏è‚É£ Get feature contributions (loadings)
# ===============================
loadings = pd.DataFrame(pca.components_.T,
                        index=X_imputed_df.columns,
                        columns=[f'PC{i+1}' for i in range(n_components)])

# Top 5 features contributing to PC1
top_features_pc1 = loadings['PC1'].abs().sort_values(ascending=False).head(5)
print("Top 5 features contributing to PC1:\n")
for i, feature in enumerate(top_features_pc1.index):
    print(f"{i+1}. {feature} (loading: {top_features_pc1[feature]:.3f})")

# Top 5 features contributing to PC2
top_features_pc2 = loadings['PC2'].abs().sort_values(ascending=False).head(5)
print("\nTop 5 features contributing to PC2:\n")
for i, feature in enumerate(top_features_pc2.index):
    print(f"{i+1}. {feature} (loading: {top_features_pc2[feature]:.3f})")

# ===============================
# 5Ô∏è‚É£ Visualize participants in 2D PCA space
# ===============================
plt.figure(figsize=(8,6))
scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Depression_PHQ9'], cmap='viridis', alpha=0.7)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA - 2 Components')
plt.grid(True)
plt.colorbar(scatter, label='Depression_PHQ9')
plt.show()

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd

# ===============================
# 1Ô∏è‚É£ Standardize features
# ===============================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed_df)

# ===============================
# 2Ô∏è‚É£ Apply PCA
# ===============================
n_components = 2
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# ===============================
# 3Ô∏è‚É£ Get feature contributions (loadings)
# ===============================
loadings = pd.DataFrame(pca.components_.T,
                        index=X_imputed_df.columns,
                        columns=[f'PC{i+1}' for i in range(n_components)])

# Top 5 features contributing to PC1
top_features_pc1 = loadings['PC1'].abs().sort_values(ascending=False).head(5)
print("Top 5 features contributing to PC1:\n")
for i, feature in enumerate(top_features_pc1.index):
    print(f"{i+1}. {feature} (loading: {top_features_pc1[feature]:.3f})")

# Top 5 features contributing to PC2
top_features_pc2 = loadings['PC2'].abs().sort_values(ascending=False).head(5)
print("\nTop 5 features contributing to PC2:\n")
for i, feature in enumerate(top_features_pc2.index):
    print(f"{i+1}. {feature} (loading: {top_features_pc2[feature]:.3f})")

# ===============================
# 4Ô∏è‚É£ Visualize participants in 2D PCA space
# ===============================
plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7, color='blue')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA - 2 Components (Features Only)')
plt.grid(True)
plt.axhline(0, color='grey', linewidth=0.5)
plt.axvline(0, color='grey', linewidth=0.5)
plt.show()

"""# Heatmap of Ordinal Social Isolation Indicators

# Checking Class Imbalance
"""

# Check class imbalance for target variables
print("\nDepression_PHQ9 class distribution:")
print(df['Depression_PHQ9'].value_counts())
print("\nLoneliness_Level class distribution:")
print(df['Loneliness_Level'].value_counts())

"""Data Augmentation"""

from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# 1Ô∏è‚É£ Define target variable
target = 'Depression_PHQ9'

# Remove rows where target is NaN
df_clean = df.dropna(subset=[target])

# Separate features and target
X = df_clean.drop(columns=[target])
y = df_clean[target]

# 2Ô∏è‚É£ Impute missing values in features
# (fills numeric NaNs with mean, categorical with most frequent)
imputer = SimpleImputer(strategy='most_frequent')
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# 3Ô∏è‚É£ Original Class Distribution
print("Before Balancing:")
print(y.value_counts())

plt.figure(figsize=(6,4))
sns.countplot(x=y)
plt.title("Original Class Distribution")
plt.show()

# 4Ô∏è‚É£ Random Oversampling
ros = RandomOverSampler(random_state=42)
X_ros, y_ros = ros.fit_resample(X_imputed, y)

print("\nAfter Random Oversampling:")
print(y_ros.value_counts())

plt.figure(figsize=(6,4))
sns.countplot(x=y_ros)
plt.title("After Random Oversampling")
plt.show()

# 5Ô∏è‚É£ SMOTE
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_imputed, y)

print("\nAfter SMOTE:")
print(y_smote.value_counts())

plt.figure(figsize=(6,4))
sns.countplot(x=y_smote)
plt.title("After SMOTE")
plt.show()

# 6Ô∏è‚É£ ADASYN Adaptive Synthetic Sampling)
adasyn = ADASYN(random_state=42)
X_adasyn, y_adasyn = adasyn.fit_resample(X_imputed, y)

print("\nAfter ADASYN:")
print(y_adasyn.value_counts())

plt.figure(figsize=(6,4))
sns.countplot(x=y_adasyn)
plt.title("After ADASYN")
plt.show()

# 7Ô∏è‚É£ Random Undersampling
rus = RandomUnderSampler(random_state=42)
X_rus, y_rus = rus.fit_resample(X_imputed, y)

print("\nAfter Random Undersampling:")
print(y_rus.value_counts())

plt.figure(figsize=(6,4))
sns.countplot(x=y_rus)
plt.title("After Random Undersampling")
plt.show()

"""**OR**"""

from imblearn.over_sampling import SMOTE
from collections import Counter

# X = features, y = target
print("Before SMOTE:", Counter(Y_cleaned))

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_imputed_df, Y_cleaned)

print("After SMOTE:", Counter(y_resampled))

"""# Feature Engineering"""

X = df.drop(columns=["Depression_PHQ9"])
Y = df["Depression_PHQ9"]

# from sklearn.preprocessing import StandardScaler
 # scaler = StandardScaler()
 # X_scaled = scaler.fit_transform(X)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
import pandas as pd

# Drop rows with NaN in the target variable Y
X_cleaned = X[Y.notna()]
Y_cleaned = Y.dropna()

# Drop columns with any NaNs from original X
X_cleaned = X_cleaned.dropna(axis=1)

# Impute (just to be sure, in case some rows still have NaNs)
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_cleaned)

# Convert to DataFrame with correct columns
X_imputed_df = pd.DataFrame(X_imputed, columns=X_cleaned.columns)

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed_df)

# Feature selection
selector = SelectKBest(score_func=f_classif, k='all')
X_selected = selector.fit_transform(X_scaled, Y_cleaned)

# Get selected feature names
selected_features = X_imputed_df.columns[selector.get_support()]
print("Selected features:\n", selected_features)

"""*****feature imp"""

from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=10)  # Select top 10 features

#drop rows with null values
X_cleaned = X[Y.notna()]
Y_cleaned = Y.dropna()
X_selected = selector.fit_transform(X_scaled, Y_cleaned)

# Get selected feature names
selected_feature_names = X_imputed_df.columns[selector.get_support()]
print("Top 10 Selected Features:\n", selected_feature_names)

df["Depression_Total"] = df["Depression_PHQ9"] + df["Depression_BDI"] + df["Depression_CESD"]

df['Depression_Total']

import pandas as pd
import numpy as np
from scipy import stats
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
# ============================
# 1Ô∏è‚É£ Identify numeric columns
# ============================
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
print("Numeric Columns:", numeric_cols)

# ============================
# 2Ô∏è‚É£ Univariate Outlier Detection (IQR method)
# ============================
outlier_flags_iqr = pd.DataFrame(index=df.index)

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    outlier_flags_iqr[col] = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR))

# Optional: sum of outlier flags per row
df['num_outliers_iqr'] = outlier_flags_iqr.sum(axis=1)

print("Rows with outliers (IQR method):")
print(df[df['num_outliers_iqr'] > 0])

# ============================
# 3Ô∏è‚É£ Multivariate Outlier Detection (Isolation Forest)
# ============================
iso = IsolationForest(contamination=0.05, random_state=42)
df['outlier_iso'] = iso.fit_predict(df[numeric_cols])
# -1 = outlier, 1 = normal

print("Rows flagged as outliers by Isolation Forest:")
print(df[df['outlier_iso'] == -1])

# ============================
# 4Ô∏è‚É£ Visualization (optional)
# ============================
for col in numeric_cols:
    plt.figure(figsize=(8,4))
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

# ============================
# 5Ô∏è‚É£ Optional: Remove outliers
# ============================
# Keep only rows that are normal according to Isolation Forest
df_cleaned = df[df['outlier_iso'] != -1].reset_index(drop=True)
print(f"Shape after removing outliers: {df_cleaned.shape}")

df = df.drop(columns=['Cluster', 'has_outlier'], errors='ignore')

df.info();

"""#Normalization-Z score"""

df_original = df.copy()
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

if 'Depression_PHQ9' in numerical_cols:
  numerical_cols.remove('Depression_PHQ9')

if 'Loneliness_Level' in numerical_cols:
  numerical_cols.remove('Loneliness_Level')
if 'Depression_Total' in numerical_cols:
  numerical_cols.remove('Depression_Total')

df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

print("\nDataFrame after Z-score standardization:")
print(df.head())
print("\nDescriptive statistics after standardization:")
print(df[numerical_cols].describe())

"""# MIN_MAX"""

from sklearn.preprocessing import MinMaxScaler

minmax = MinMaxScaler()
df_minmax = df_original.copy()
df_minmax[numerical_cols] = minmax.fit_transform(df_original[numerical_cols])

df_minmax[numerical_cols].head()

"""# Robust Scaler"""

from sklearn.preprocessing import RobustScaler

robust = RobustScaler()
df_robust = df_original.copy()
df_robust[numerical_cols] = robust.fit_transform(df_original[numerical_cols])

df_robust[numerical_cols].head()

print("Z-Score mean:", df[numerical_cols].mean().mean())
print("Z-Score std:", df[numerical_cols].std().mean())

print("Min-Max min:", df_minmax[numerical_cols].min().min(),
      " max:", df_minmax[numerical_cols].max().max())

print("Robust median:", df_robust[numerical_cols].median().median())

"""# DETECT OUTLIERS"""

import numpy as np
df_original = df.copy()

outlier_counts = {}

for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    outliers = df[(df[col] < lower) | (df[col] > upper)][col]
    outlier_counts[col] = len(outliers)

# Show count of outliers per column
outlier_counts

outlier_cols = [col for col, count in outlier_counts.items() if count > 0]

import warnings
warnings.filterwarnings("ignore")
df_original = df.copy()

for col in numerical_cols:
    plt.figure(figsize=(4, 3))
    plt.boxplot(df[col], vert=True)
    plt.title(f'Box Plot of {col}')
    plt.ylabel(col)
    plt.tight_layout()
    plt.show()

"""# CAPING"""

from scipy.stats import zscore

z_scores = df[numerical_cols].apply(zscore)
outliers_z = (np.abs(z_scores) > 3)
outliers_z.sum()

df_capped = df.copy()

for col in z_scores.columns:
    col_z = z_scores[col]

    upper = df[col][col_z <= 3].max()
    lower = df[col][col_z >= -3].min()

    df_capped[col] = np.where(
        col_z > 3, upper,
        np.where(col_z < -3, lower, df[col])
    )

df_capped[z_scores.columns].describe()

from scipy.stats import zscore

z_after = df_capped[z_scores.columns].apply(zscore)
(np.abs(z_after) > 3).sum()

"""# Remove outliers"""

import numpy as np

numerical_cols = df.select_dtypes(include='number').columns.tolist()

# Boolean mask for rows that have any outlier
iqr_row_mask = np.zeros(len(df), dtype=bool)

for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    # Mark rows with outliers in this column
    iqr_row_mask |= (df[col] < lower) | (df[col] > upper)

df_removed = df.loc[~iqr_row_mask].copy()

removed_rows = df.loc[iqr_row_mask].copy()

print("Removed Rows (Outliers):")
print("Total removed:", len(removed_rows))
display(removed_rows.head(10))  # show first 10 for preview

left_rows = df.loc[~iqr_row_mask].copy()

print("Rows Left After Removing Outliers:")
print("Total left:", len(left_rows))
display(left_rows.head(10))  # preview

"""#clustering techniques"""

# This is your final clean feature matrix
clustering_df = X_scaled
import numpy as np

print(np.isnan(clustering_df).sum())
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []

for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(clustering_df)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel("Number of clusters (k)")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

"""# Silhouette Score"""

from sklearn.metrics import silhouette_score

silhouette_scores = []

for k in range(2, 11):   # silhouette starts from k=2
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(clustering_df)
    score = silhouette_score(clustering_df, labels)
    silhouette_scores.append(score)
    print(f"Silhouette score for k={k}: {score:.4f}")

plt.figure(figsize=(8,5))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.xlabel("Number of clusters (k)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Analysis")
plt.grid(True)
plt.show()

# 1Ô∏è‚É£ Fit KMeans and get cluster labels
optimal_k = 3

kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(clustering_df)

# 2Ô∏è‚É£ Create a dataframe that matches clustering_df
clustered_df = pd.DataFrame(
    clustering_df,
    columns=X_imputed_df.columns
)

# 3Ô∏è‚É£ Add cluster labels
clustered_df['Cluster'] = cluster_labels

# 4Ô∏è‚É£ Cluster-wise summary
cluster_summary = clustered_df.groupby('Cluster').mean()
print(cluster_summary)

from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=1.5, min_samples=5)
db_labels = dbscan.fit_predict(clustering_df)
cluster_results = pd.DataFrame(clustering_df)
cluster_results['DBSCAN_Cluster'] = db_labels
cluster_results['DBSCAN_Cluster'].value_counts()

"""# DENDOGRAM"""

clustering_df = X_scaled
from scipy.cluster.hierarchy import linkage, fcluster
import pandas as pd

# Step 1: Build hierarchical clustering tree
linked = linkage(clustering_df, method='ward')

# Step 2: Cut the tree to get cluster labels (e.g., 3 clusters)
hierarchical_labels = fcluster(linked, t=3, criterion='maxclust')

# Step 3: Create dataframe with hierarchical cluster labels
hierarchical_df = pd.DataFrame(clustering_df)
hierarchical_df['Hierarchical_Cluster'] = hierarchical_labels

# Check cluster sizes
print(hierarchical_df['Hierarchical_Cluster'].value_counts())

from scipy.cluster.hierarchy import dendrogram
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
dendrogram(
    linked,
    truncate_mode='lastp',  # shows only last clusters
    p=10,
    leaf_rotation=90,
    leaf_font_size=10
)
plt.title("Dendrogram (Hierarchical Clustering)")
plt.xlabel("Cluster size")
plt.ylabel("Distance")
plt.show()

"""# Spliting the data"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# split the dataset into 70-30 to train the models

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")
print(f"Training target distribution:\n{y_train.value_counts(normalize=True)}")
print(f"Testing target distribution:\n{y_test.value_counts(normalize=True)}")

# split the dataset into 75-25 to train the models

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42, stratify=y_resampled)

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")
print(f"Training target distribution:\n{y_train.value_counts(normalize=True)}")
print(f"Testing target distribution:\n{y_test.value_counts(normalize=True)}")

# split the dataset into 80-20 to train the models

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")
print(f"Training target distribution:\n{y_train.value_counts(normalize=True)}")
print(f"Testing target distribution:\n{y_test.value_counts(normalize=True)}")

"""#k-fold cross-validation."""

from sklearn.model_selection import KFold, cross_val_score
n_splits = 5

kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)


model = RandomForestClassifier(n_estimators=100, random_state=42)

scores = cross_val_score(model, X_resampled, y_resampled, cv=kf, scoring='accuracy')

print(f"Accuracy scores for each fold: {scores}")
print(f"Mean cross-validation accuracy: {scores.mean():.4f}")
print(f"Standard deviation of cross-validation accuracy: {scores.std():.4f}")

"""# creating a validation set

"""

#validation set

X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp) # Split temp into val and test

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Validation set size: {X_val.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")

print(f"Training target distribution:\n{y_train.value_counts(normalize=True)}")
print(f"Validation target distribution:\n{y_val.value_counts(normalize=True)}")
print(f"Testing target distribution:\n{y_test.value_counts(normalize=True)}")

"""# identifying the classification model and training them on pre processed dataset"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis


models = {
  'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
  'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
  'Support Vector Machine': SVC(random_state=42),
  'K-Nearest Neighbors': KNeighborsClassifier(),
  'Decision Tree': DecisionTreeClassifier(random_state=42),
  'Gaussian Naive Bayes': GaussianNB(),
  'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),
  'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis()
}


results = {}
for name, model in models.items():
  print(f"\nTraining {name}...")
  model.fit(X_train, y_train)

  y_pred = model.predict(X_test)


  print(f"Evaluating {name}:")
  print("Classification Report:")
  print(classification_report(y_test, y_pred))

  print("Confusion Matrix:")
  print(confusion_matrix(y_test, y_pred))


  results[name] = {
    'model': model,
    'classification_report': classification_report(y_test, y_pred, output_dict=True),
    'confusion_matrix': confusion_matrix(y_test, y_pred)
  }

"""# training the model"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Support Vector Machine': SVC(random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Gaussian Naive Bayes': GaussianNB(),
    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),
    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis()
}

results = {}
for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    print(f"Evaluating {name}:")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    results[name] = {
        'model': model,
        'classification_report': classification_report(y_test, y_pred, output_dict=True),
        'confusion_matrix': confusion_matrix(y_test, y_pred)
    }

"""# evaluating the model"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

print("\n--- Model Evaluation Summary ---")

for name, evaluation_data in results.items():
    print(f"\n### {name} ###")
    print("Classification Report:")
    report_str = ""
    report_dict = evaluation_data['classification_report']
    report_str += "{:<15} {:<10} {:<10} {:<10} {:<10}\n".format("", "precision", "recall", "f1-score", "support")
    report_str += "{:<15} {:<10} {:<10} {:<10} {:<10}\n".format("", "---------", "---------", "---------", "---------")

    for label, metrics in report_dict.items():
        if isinstance(metrics, dict):
            report_str += "{:<15} {:<10.2f} {:<10.2f} {:<10.2f} {:<10}\n".format(
                str(label), metrics['precision'], metrics['recall'], metrics['f1-score'], metrics['support']
            )
        elif isinstance(metrics, float):
            # Print accuracy separately
            report_str += "\n{:<15} {:<10.2f}\n".format(str(label), metrics)

    print(report_str)

    print("Confusion Matrix:")
    print(evaluation_data['confusion_matrix'])